{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRelevancePiterkin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1vTQi2K5C9NnNXrtTaCgKRw7oI07DK1Im",
      "authorship_tag": "ABX9TyOaFHXOD2l55Zj7wkNpQ+mi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MitPitt/IR/blob/master/TextRelevancePiterkin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVpumQ1SBogn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stanza\n",
        "!pip install spacy_stanza\n",
        "!pip install pymorphy2==0.8\n",
        "!pip install transliterate\n",
        "!pip install googletrans\n",
        "!pip install yandex-translater\n",
        "!pip install pyaspeller"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxoiycrH0iG1",
        "colab_type": "code",
        "outputId": "7585e45f-97b0-4cb5-884e-1acd6e5bf316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import googletrans\n",
        "import yandex.Translater\n",
        "import pyaspeller\n",
        "from transliterate import translit\n",
        "import stanza\n",
        "from spacy_stanza import StanzaLanguage\n",
        "\n",
        "stanza.download('ru') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 10.3MB/s]                    \n",
            "2020-05-20 01:03:46 INFO: Downloading default packages for language: ru (Russian)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/ru/default.zip: 100%|██████████| 591M/591M [01:47<00:00, 5.52MB/s]\n",
            "2020-05-20 01:05:42 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubq_17fB3-6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QueryProcessor():\n",
        "    def __init__(self):\n",
        "        # layouts\n",
        "        self._rus_chars = \"ёйцукенгшщзхъфывапролджэячсмитьбю.Ё!\\\"№;%:?*()_+ЙЦУКЕНГШЩЗХЪ/ФЫВАПРОЛДЖЭЯЧСМИТЬБЮ,\" # note \\\"\n",
        "        self._eng_chars = \"`qwertyuiop[]asdfghjkl;'zxcvbnm,./~!@#$%^&*()_+QWERTYUIOP{}|ASDFGHJKL:\\\"ZXCVBNM<>?\" # note \\\"\n",
        "        self._trans_ru2en = str.maketrans(self._rus_chars, self._eng_chars)\n",
        "        self._trans_en2ru = str.maketrans(self._eng_chars, self._rus_chars)\n",
        "        #translator 1\n",
        "        self.yandex_tr = yandex.Translater.Translater()\n",
        "        self.yandex_tr.set_key('trnsl.1.1.20200508T141638Z.ad4b0fa3731d6994.9453309fdac5d82ddf65137156a065683eeb4fe9')\n",
        "        #translator 2\n",
        "        self.google_tr = googletrans.Translator()\n",
        "        #spellchecker\n",
        "        self.spellchecker = pyaspeller.YandexSpeller(lang=['ru','en'], ignore_capitalization=True, ignore_urls=True, ignore_digits=True,)\n",
        "        #lemmatizer\n",
        "        self.nlp = StanzaLanguage(stanza.Pipeline(lang=\"ru\"))\n",
        "\n",
        "    def _check_layout(self, query):\n",
        "        det = self.google_tr.detect(query)\n",
        "        query_ru = query.translate(self._trans_en2ru)\n",
        "        det_ru = self.google_tr.detect(query_ru)\n",
        "        #print(det_ru.confidence, det.confidence)\n",
        "        if det.lang!='ru' and det.lang!='uk' and det_ru.confidence > det.confidence:\n",
        "            query = query_ru \n",
        "        return query\n",
        "\n",
        "    def _translate(self, query):\n",
        "        alternatives = []\n",
        "        if self.google_tr.detect(query).lang != 'ru':\n",
        "            self.yandex_tr.set_text(query)\n",
        "            self.yandex_tr.set_from_lang(self.yandex_tr.detect_lang())\n",
        "            self.yandex_tr.set_to_lang('ru')\n",
        "            alternatives.append(self.yandex_tr.translate().lower())\n",
        "            #gtr = self.google_tr.translate(query, dest='ru').text.lower()\n",
        "            #if gtr not in alternatives:\n",
        "            #    alternatives.append(gtr)\n",
        "        else:\n",
        "            alternatives.append(query)\n",
        "        return alternatives\n",
        "\n",
        "    def _spellcheck(self, query):\n",
        "        for error in self.spellchecker.spell(query):\n",
        "            #print(error)\n",
        "            if len(error['s'])>0:\n",
        "                query = query.replace(error['word'], error['s'][0], 1)\n",
        "            else:\n",
        "                word = translit(error['word'], language_code='ru', reversed=True)\n",
        "                try:\n",
        "                    www = next(self.spellchecker.spell(word))['s']\n",
        "                    if len(www) > 0:\n",
        "                        query = query.replace(error['word'], www[0], 1)\n",
        "                except StopIteration:\n",
        "                    pass\n",
        "        return query\n",
        "\n",
        "    def _alts(self, query, swaplang='en'):\n",
        "        \"\"\"\n",
        "        get synonymous queries\n",
        "        \"\"\"\n",
        "        self.yandex_tr.set_text(query)\n",
        "        self.yandex_tr.set_from_lang('ru')\n",
        "        self.yandex_tr.set_to_lang(swaplang)\n",
        "        self.yandex_tr.set_text(self.yandex_tr.translate())\n",
        "        self.yandex_tr.set_from_lang(swaplang)\n",
        "        self.yandex_tr.set_to_lang('ru')\n",
        "        alts = [\n",
        "                query,\n",
        "                self.yandex_tr.translate().lower(),\n",
        "                self.google_tr.translate((self.google_tr.translate(query, dest=swaplang).text), dest='ru').text.lower(),\n",
        "                ]\n",
        "        return alts\n",
        "\n",
        "    def _lemmatize(self, query):\n",
        "        lemmas = self.nlp(query)\n",
        "        #for word in lemmas:\n",
        "        #    print(word.lemma_, word.pos_)\n",
        "        query = [word.lemma_ for word in lemmas if (word.pos_ != 'PUNCT' and word.pos_ !='ADP' and word.pos_ !='ADV' and word.pos_ !='PART' and word.pos_ !='CCONJ')] # delete punctuation and stopwords\n",
        "        return query\n",
        "\n",
        "    def process(self, query):\n",
        "        query = self._check_layout(query)\n",
        "        alternatives = self._translate(query)\n",
        "        alternatives = [self._spellcheck(query) for query in alternatives]\n",
        "        #print(alternatives)\n",
        "        newalts = []\n",
        "        languages = ['en'] # ,es,de\n",
        "        for lang in languages:\n",
        "            alternatives_trick =  [self._alts(query, swaplang=lang) for query in alternatives]\n",
        "            alternatives_trick = {item for sublist in alternatives_trick for item in sublist} #flatten and remove dupes\n",
        "            #print(alternatives_trick)\n",
        "            newalts += list(alternatives_trick)\n",
        "        alternatives = newalts\n",
        "        #print(alternatives)\n",
        "        alternatives = [self._lemmatize(query) for query in alternatives]\n",
        "        #query = {item for sublist in alternatives for item in sublist} # merge all uqique lemmas into one query\n",
        "        #return ' '.join(query)\n",
        "        return alternatives"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RIYpVLNk9QU",
        "colab_type": "code",
        "outputId": "44649f5b-1d61-46bf-c0ae-108fa7ddc71b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "Processor = QueryProcessor()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-20 01:05:49 INFO: Loading these models for language: ru (Russian):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | syntagrus |\n",
            "| pos       | syntagrus |\n",
            "| lemma     | syntagrus |\n",
            "| depparse  | syntagrus |\n",
            "| ner       | wikiner   |\n",
            "=========================\n",
            "\n",
            "2020-05-20 01:05:49 INFO: Use device: cpu\n",
            "2020-05-20 01:05:49 INFO: Loading: tokenize\n",
            "2020-05-20 01:05:49 INFO: Loading: pos\n",
            "2020-05-20 01:05:50 INFO: Loading: lemma\n",
            "2020-05-20 01:05:50 INFO: Loading: depparse\n",
            "2020-05-20 01:05:51 INFO: Loading: ner\n",
            "2020-05-20 01:05:53 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4VK7E7U3T_o",
        "colab_type": "code",
        "outputId": "5970a6a0-5d88-46d6-a888-b52e455c9fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "Processor.process('как файл пдф перевести в бмп') # странно лемматизирует \"пдф\" но это норм, в документах лемматизирует так же"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['как', 'файл', 'пдф', 'перевести', 'бмп'],\n",
              " ['как', 'файл', 'п+f', 'конвертировать', 'bmp'],\n",
              " ['как', 'файл', 'формат', 'п+f', 'конвертировать', 'bmp']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NroPUfKa0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=0\n",
        "\n",
        "with open('/content/drive/My Drive/text relevance/lemmatized_queries.txt', mode='a') as  lemqueries:\n",
        "    with open('/content/drive/My Drive/text relevance/queries.numerate.txt', encoding='utf8') as queries:\n",
        "        for line in queries:\n",
        "            query_id, query = line.strip().split('\\t')\n",
        "            print(\"({0})\\t{1}\".format(query_id, query))\n",
        "            lemd = Processor.process(query)\n",
        "            print(lemd)\n",
        "            for lem in lemd:\n",
        "                lemqueries.write(query_id +'\\t'+ ' '.join(lem) +'\\n')\n",
        "            i+=1\n",
        "i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PeXzome1B75",
        "colab_type": "code",
        "outputId": "ba480a18-25c0-4564-9e1e-9bd060dde44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "nlp = StanzaLanguage(stanza.Pipeline(lang=\"ru\", use_gpu=True, processors='tokenize,pos,lemma'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-18 10:43:17 INFO: Loading these models for language: ru (Russian):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | syntagrus |\n",
            "| pos       | syntagrus |\n",
            "| lemma     | syntagrus |\n",
            "=========================\n",
            "\n",
            "2020-05-18 10:43:17 INFO: Use device: gpu\n",
            "2020-05-18 10:43:17 INFO: Loading: tokenize\n",
            "2020-05-18 10:43:27 INFO: Loading: pos\n",
            "2020-05-18 10:43:28 INFO: Loading: lemma\n",
            "2020-05-18 10:43:29 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xtf6UC2ZeCH",
        "colab_type": "code",
        "outputId": "32ca6cc2-c84b-4692-a4b4-f9b97d65f108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for i in nlp('куда поехать отдыхать'):\n",
        "  print(i.lemma_, i.pos_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "куда ADV\n",
            "поехать VERB\n",
            "отдыхать VERB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJTk9f4nZzUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "regex = re.compile('[^а-яА-Яa-zA-Z0-9]')\n",
        "\n",
        "def parse_html(filepath):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            \n",
        "            contents = f.read()\n",
        "\n",
        "            soup = BeautifulSoup(contents, 'html.parser')\n",
        "\n",
        "            try:\n",
        "                title = soup.title.text.lower()\n",
        "            except AttributeError:\n",
        "                return [''], ['']\n",
        "            \n",
        "            data = soup.findAll(text=re.compile('[а-яА-Я]'))\n",
        "            def visible(element):\n",
        "                #print(str(element.encode('utf-8')))\n",
        "                if element.parent.name in ['style', 'script', '[document]']:\n",
        "                    return False\n",
        "                elif re.search('<.*>', str(element.encode('utf-8',errors='surrogatepass'))):\n",
        "                    return False\n",
        "                return True\n",
        "            other_text = filter(visible, data)\n",
        "            other_text = [x.strip() for x in other_text]\n",
        "            other_textt = []\n",
        "            for x in other_text:\n",
        "                x = regex.sub(' ', x).strip()\n",
        "                if len(x)>1:\n",
        "                    other_textt.extend([word.lemma_.lower() for word in nlp(x) if len(word.lemma_)>1 and (word.pos_ != 'PUNCT' and word.pos_ !='ADP' and word.pos_ !='ADV' and word.pos_ !='PART' and word.pos_ !='CCONJ')])\n",
        "            \n",
        "            \n",
        "            title = [word.lemma_.lower() for word in nlp(title) if len(word.lemma_)>1 and (word.pos_ != 'PUNCT' and word.pos_ !='ADP' and word.pos_ !='ADV' and word.pos_ !='PART' and word.pos_ !='CCONJ')]\n",
        "    return title, other_textt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl7n4v0YHWFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parse_html(\"/content/drive/My Drive/text relevance/content/content/20170702/doc.0000.dat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odISrCygk4qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAzqiU81ikb1",
        "colab_type": "code",
        "outputId": "aa2e2b1b-f0fd-4499-c3b3-27d3eccc368f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "id_to_url = pd.read_csv(\"/content/drive/My Drive/text relevance/urls.numerate.txt\", sep='\\t', encoding='utf-8', header=None, names=['id','url','lemmatized_text_path'])\n",
        "id_to_url.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>lemmatized_text_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>eva.ru/forum/topic-tree-mobile.htm?messageId=8...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>e1.ru/articles/travel/page_4/009/994/article_9...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>bikepost.ru/qa/post/9065/Kak-pravilno-sdelat-v...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>rutax.polpred.com/?ns=1&amp;page=2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>coollib.com/b/259649/read</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                                url  lemmatized_text_path\n",
              "0   1  eva.ru/forum/topic-tree-mobile.htm?messageId=8...                   NaN\n",
              "1   2  e1.ru/articles/travel/page_4/009/994/article_9...                   NaN\n",
              "2   3  bikepost.ru/qa/post/9065/Kak-pravilno-sdelat-v...                   NaN\n",
              "3   4                     rutax.polpred.com/?ns=1&page=2                   NaN\n",
              "4   5                          coollib.com/b/259649/read                   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_Loe8o9QthD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=0\n",
        "for root, dirs, files in os.walk(\"/content/drive/My Drive/text relevance/content/content\"):\n",
        "  if root == '/content/drive/My Drive/text relevance/content/content/20170711':\n",
        "    for filee in files:\n",
        "        i+=1\n",
        "        filepath = root+'/'+filee\n",
        "\n",
        "        lemmatized_path = root + '_lem/'+ filee + '.lemmatized.txt'\n",
        "        \n",
        "        #if os.path.getsize(filepath) > 1000000:\n",
        "        #  continue\n",
        "\n",
        "        if not os.path.isfile(lemmatized_path):\n",
        "          \n",
        "            with open(filepath, 'rb') as f:\n",
        "                url = f.readline().rstrip()\n",
        "            print(i)\n",
        "            print('URL:', url)\n",
        "            print(lemmatized_path)\n",
        "            lemmatized_title, lemmatized_text = parse_html(filepath)\n",
        "            print('title:', ' '.join(lemmatized_title))\n",
        "            with open(lemmatized_path, 'w') as the_file:\n",
        "                the_file.write(' '.join(lemmatized_title) + '\\n' + ' '.join(lemmatized_text))\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        id_to_url.loc[id_to_url['url'] == url, 'lemmatized_text_path'] = lemmatized_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIdv3QmtQthl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d853kKEqrR0x",
        "colab_type": "code",
        "outputId": "8ca8806e-6d54-43dc-834d-d774f3669ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "id_to_url = pd.read_csv(\"/content/drive/My Drive/text relevance/urls.numerate.txt\", sep='\\t', encoding='utf-8', header=None, names=['id','url','lemmatized_text_path'])\n",
        "id_to_url.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>lemmatized_text_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>eva.ru/forum/topic-tree-mobile.htm?messageId=8...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>e1.ru/articles/travel/page_4/009/994/article_9...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>bikepost.ru/qa/post/9065/Kak-pravilno-sdelat-v...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>rutax.polpred.com/?ns=1&amp;page=2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>coollib.com/b/259649/read</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                                url  lemmatized_text_path\n",
              "0   1  eva.ru/forum/topic-tree-mobile.htm?messageId=8...                   NaN\n",
              "1   2  e1.ru/articles/travel/page_4/009/994/article_9...                   NaN\n",
              "2   3  bikepost.ru/qa/post/9065/Kak-pravilno-sdelat-v...                   NaN\n",
              "3   4                     rutax.polpred.com/?ns=1&page=2                   NaN\n",
              "4   5                          coollib.com/b/259649/read                   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcUNjhvkHhyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=0\n",
        "for root, dirs, files in os.walk(\"/content/drive/My Drive/text relevance/content/content\"):\n",
        "  if not '_lem' in root:\n",
        "    for filee in files:\n",
        "        i+=1\n",
        "        if i %100 == 0:\n",
        "            print(i)\n",
        "        filepath = root+'/'+filee\n",
        "        \n",
        "        lemmatized_path = root + '_lem/'+ filee + '.lemmatized.txt'\n",
        "        \n",
        "        if os.path.isfile(lemmatized_path): # if lemmatized version exists\n",
        "            with open(filepath, 'r', encoding='latin1') as f:\n",
        "                url = f.readline().rstrip()\n",
        "            id_to_url.loc[id_to_url['url'] == url, 'lemmatized_text_path'] = lemmatized_path\n",
        "            #print(lemmatized_path, url)\n",
        "        else:\n",
        "            continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEw3qMxUkX8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "id_to_url = pd.read_csv('/content/drive/My Drive/text relevance/main.csv', sep='\\t', encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-UdaCjH2M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_to_url.to_csv('/content/drive/My Drive/text relevance/main.csv', sep='\\t')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOyKYtJ4SQQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_to_url.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRejngxXHQzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_to_url.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UveSH634F2wT",
        "colab_type": "code",
        "outputId": "7e4880bb-a56c-4675-c45a-0187ab3c8c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-79n9Xw1Ff59",
        "colab_type": "code",
        "outputId": "a5f16984-6103-4710-c2b9-3d70c89f49af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "text = ['cant', 'citadel', 'police']\n",
        "bigrams = list(ngrams(text, 2)) \n",
        "print(' '.join(bigrams[0]))\n",
        "print(bigrams)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cant citadel\n",
            "[('cant', 'citadel'), ('citadel', 'police')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79bSYNsKQQmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "class RelevantTexts():\n",
        "    def __init__(self):\n",
        "        self.query_to_docs = pd.read_csv('/content/drive/My Drive/text relevance/sample.technosphere.ir1.textrelevance.submission.txt')\n",
        "        self.main = pd.read_csv('/content/drive/My Drive/text relevance/main.csv', sep='\\t', encoding='latin1')\n",
        "        \n",
        "        self.mean_title_len = 0\n",
        "        self.mean_doc_len = 0\n",
        "        self.doc_num = 0\n",
        "        self.idf_doc = defaultdict(float)\n",
        "        self.idf_title = defaultdict(float)\n",
        "        self.idf_doc_bi = defaultdict(float)\n",
        "        self.idf_title_bi = defaultdict(float)\n",
        "        self.k1 = 1.75\n",
        "        self.b = 0.75\n",
        "        self.title_coef = 1.5\n",
        "        self.doc_coef = 1\n",
        "        self.bigram_coef = 2\n",
        "    \n",
        "    def _relevance(self, query, doc): #bm25f ungram bigram\n",
        "        title_score = 0\n",
        "        doc_score = 0\n",
        "        title_bigram_score = 0\n",
        "        doc_bigram_score = 0\n",
        "        with open(doc, 'r') as f:\n",
        "            title = f.readline().strip()\n",
        "            if title == '':\n",
        "                return -1\n",
        "            text = f.readline().strip()\n",
        "\n",
        "            title_len = len(title.split())\n",
        "            doc_len = len(text.split())\n",
        "            \n",
        "            for word in query:\n",
        "                tf_title = title.count(word)\n",
        "                tf_doc = text.count(word)\n",
        "\n",
        "                title_score += self.idf_title[word] * (tf_title * (self.k1 + 1)) / (tf_title + self.k1 * (1 - self.b + self.b * title_len / self.mean_title_len))\n",
        "                doc_score += self.idf_doc[word] * (tf_doc * (self.k1 + 1)) / (tf_doc + self.k1 * (1 - self.b + self.b * doc_len / self.mean_doc_len))\n",
        "            \n",
        "            bigrams = list(ngrams(query, 2))\n",
        "            for bi in bigrams:\n",
        "                tf_title = title.count(' '.join(bi))\n",
        "                tf_doc = text.count(' '.join(bi))\n",
        "                title_bigram_score += self.idf_title_bi[bi] * (tf_title * (self.k1 + 1)) / (tf_title + self.k1 * (1 - self.b + self.b * title_len / self.mean_title_len))\n",
        "                doc_bigram_score += self.idf_doc_bi[bi] * (tf_doc * (self.k1 + 1)) / (tf_doc + self.k1 * (1 - self.b + self.b * doc_len / self.mean_doc_len))\n",
        "\n",
        "        #print(title)\n",
        "        #print('ungram scores for title and doc:', title_score, doc_score)\n",
        "        #print('bigram scores for title and doc:', title_bigram_score, doc_bigram_score)\n",
        "        #print()\n",
        "        score = (self.doc_coef * doc_score + self.title_coef * title_score) + self.bigram_coef * (self.title_coef * title_bigram_score + 0.5*doc_bigram_score)\n",
        "        return score\n",
        "\n",
        "    def _doc_stats(self, doc_docs):\n",
        "        sum_title_len = 0\n",
        "        sum_doc_len = 0\n",
        "        for doc in doc_docs:\n",
        "            with open(doc, 'r') as f:\n",
        "                title = f.readline().strip().split()\n",
        "                if title != []:\n",
        "                    self.doc_num += 1\n",
        "                    text = f.readline().strip().split()\n",
        "                    sum_title_len += len(title)\n",
        "                    sum_doc_len += len(text)\n",
        "\n",
        "                    words = set()\n",
        "                    for word in text:\n",
        "                      if not word in words:\n",
        "                        self.idf_doc[word] += 1\n",
        "                        words.add(word)\n",
        "\n",
        "                    words = set()\n",
        "                    bigrams = list(ngrams(text, 2))\n",
        "                    for bi in bigrams:\n",
        "                      if not bi in words:\n",
        "                        self.idf_doc_bi[bi] += 1\n",
        "                        words.add(bi)\n",
        "\n",
        "                    words = set()\n",
        "                    for word in title:\n",
        "                      if not word in words:\n",
        "                        self.idf_title[word] += 1\n",
        "                        words.add(word)\n",
        "\n",
        "                    words = set()\n",
        "                    bigrams = list(ngrams(title, 2))\n",
        "                    for bi in bigrams:\n",
        "                      if not bi in words:\n",
        "                        self.idf_title_bi[bi] += 1\n",
        "                        words.add(bi)\n",
        "\n",
        "        self.mean_title_len = sum_title_len / self.doc_num\n",
        "        self.mean_doc_len = sum_doc_len / self.doc_num\n",
        "        self._calculate_idf(self.idf_doc)\n",
        "        self._calculate_idf(self.idf_title)\n",
        "        self._calculate_idf(self.idf_doc_bi)\n",
        "        self._calculate_idf(self.idf_title_bi)\n",
        "        #print(self.idf_doc)\n",
        "        #input()\n",
        "        #print(self.idf_title)\n",
        "        #input()\n",
        "        #print(self.idf_doc_bi)\n",
        "        #input()\n",
        "        #print(self.idf_title_bi)\n",
        "        #input()\n",
        "\n",
        "    def _calculate_idf(self, some_idf):\n",
        "        for word in some_idf.keys():\n",
        "            idf = np.log( (self.doc_num - some_idf[word] +0.5) / (some_idf[word]) +0.5)\n",
        "            if idf > 0:\n",
        "                some_idf[word] = idf\n",
        "            else:\n",
        "                some_idf[word] = 0.000001\n",
        "\n",
        "    def count_scores(self, query_id, query):\n",
        "        progress = 0\n",
        "        doc_ids_with_nans = list(self.query_to_docs.loc[self.query_to_docs.QueryId == query_id].DocumentId)\n",
        "        doc_docs = []\n",
        "        doc_ids = []\n",
        "        for doc_id in doc_ids_with_nans:\n",
        "            d = self.main.loc[self.main.id == doc_id].iloc[0].lemmatized_text_path\n",
        "            if isinstance(d, str):\n",
        "                doc_docs.append(d)\n",
        "                doc_ids.append(doc_id)\n",
        "        query = query.split()\n",
        "        relevancies = {}\n",
        "\n",
        "        self.mean_title_len = 0\n",
        "        self.mean_doc_len = 0\n",
        "        self.doc_num = 0\n",
        "        self.idf_title = defaultdict(float)\n",
        "        self.idf_doc = defaultdict(float)\n",
        "        self.idf_doc_bi = defaultdict(float)\n",
        "        self.idf_title_bi = defaultdict(float)\n",
        "        self._doc_stats(doc_docs)\n",
        "        #print(self.mean_title_len, self.mean_doc_len, self.doc_num)\n",
        "        #input()\n",
        "        #print(self.idf_title)\n",
        "        #input()\n",
        "\n",
        "        for doc, doc_id in zip(doc_docs, doc_ids):\n",
        "            progress += 1\n",
        "            rel = self._relevance(query, doc)\n",
        "            relevancies[doc_id] = rel\n",
        "        c = Counter(relevancies)\n",
        "        #print(c)\n",
        "        #return c.most_common(10)\n",
        "        return c\n",
        "\n",
        "    def see_docs(self, query_id):\n",
        "        doc_ids_with_nans = list(self.query_to_docs.loc[self.query_to_docs.QueryId == query_id].DocumentId)\n",
        "        doc_docs = []\n",
        "        doc_ids = []\n",
        "        for doc_id in doc_ids_with_nans:\n",
        "            d = self.main.loc[self.main.id == doc_id].iloc[0].lemmatized_text_path\n",
        "            if isinstance(d, str):\n",
        "                doc_docs.append(d)\n",
        "                doc_ids.append(doc_id)\n",
        "        for doc in doc_docs:\n",
        "            with open(doc, 'r') as f:\n",
        "                title = f.readline().strip()\n",
        "                print(doc[-41:], title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci29e8CWR0VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q = RelevantTexts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESCKvFRdjKcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q.see_docs(159)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6WfK4m8TJnU",
        "colab_type": "code",
        "outputId": "a78e1ab3-8ea2-4d6b-87f9-8d57ea40bb53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "a = q.count_scores(159, 'увеличить предмет симс 4')\n",
        "b = q.count_scores(159, 'увеличить предмет sims 4')\n",
        "top = (a + b).most_common(10)\n",
        "\n",
        "print(top)\n",
        "\n",
        "for i, doc_id in enumerate(top):\n",
        "    #print(q.main.loc[q.main.id == doc_id[0]].iloc[0].lemmatized_text_path[-41:])\n",
        "    with open(q.main.loc[q.main.id == doc_id[0]].iloc[0].lemmatized_text_path, 'r') as f:\n",
        "        title = f.readline().strip()\n",
        "        print(i, doc_id[0], title)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(15306, 99.18632706615082), (15335, 90.1278978233102), (15296, 70.54351839240063), (15360, 65.18344177169456), (15303, 64.15169229701168), (15340, 48.96807629401731), (15324, 43.57524117592218), (15382, 41.985036673343004), (15326, 41.2827941937404), (15371, 41.12074239621655)]\n",
            "0 15306 увеличить предмет sims4\n",
            "1 15335 ответыдmail.ru увеличить предмет sims4\n",
            "2 15296 увеличить предмет thesims4 youtube\n",
            "3 15360 увеличить предмет the sims\n",
            "4 15303 увеличить предмет игра sims\n",
            "5 15340 увеличение предмет симс youtube\n",
            "6 15324 увеличивать предмет симс видео wikibit.me\n",
            "7 15382 увеличить симс количество персонаж семья\n",
            "8 15326 увеличивать предмет симс youtube\n",
            "9 15371 увеличить любой вещь симс работа youtube\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMDUBVKYAQ7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/text relevance/submission.txt', 'w') as sub:\n",
        "    sub.write('QueryId,DocumentId\\n')\n",
        "    for i in range(1,400):\n",
        "        alts = []\n",
        "        with open('/content/drive/My Drive/text relevance/lemmatized_queries.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                query_id, query = line.strip().split('\\t')\n",
        "                if int(query_id) == i:\n",
        "                    alts.append(query)\n",
        "                    print(int(query_id), i, query)\n",
        "        a = q.count_scores(i, alts[0])\n",
        "        for alt in alts[1:]:\n",
        "          b = q.count_scores(i, alt)\n",
        "          a = a + b\n",
        "\n",
        "        top = (a).most_common(10) ### top 10\n",
        "        for doc_id in top:\n",
        "            sub.write( str(i) + ',' + str(doc_id[0]) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH6gTqgELSa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}